{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from the notebook found at [How to Build a Law Bot](https://lawyerist.com/how-build-law-bot/)\n",
    "\n",
    "## Install libraries\n",
    "\n",
    "If you haven't already, you may need to install some dependencies. On the command line, run the following to install/update gspread, oauth2client, PyOpenSSL, and python-twitter.\n",
    "```\n",
    "pip install gspread\n",
    "pip install --upgrade oauth2client\n",
    "pip install PyOpenSSL\n",
    "pip install python-twitter\n",
    "```\n",
    "Library installs are one and done. So after doing this once, you should be all set. \n",
    "\n",
    "## Import modules and set variables\n",
    "\n",
    "Now we're getting into the bot's code. This is what will run every time your bot is called. \n",
    "\n",
    "You will need to create a new Google Sheet (same instructions as [last time](https://lawyerist.com/126074/online-forms-meet-local-document-automation-cut-and-paste-coding/)). Delete rows 2-999. This is because the code below appends values to the end of your sheet. So if you fail to remove rows 2-999, values will be appended to row 1000. Additionally, it looks at the last row of the sheet for your old values. So right off the bat it will be looking at your one solitary row. Also, delete columns D through Z to avoind having to print a bunch of empty columns.\n",
    "\n",
    "As for a Twitter account and Twitter credentials, follow the instruction in [this post](https://lawyerist.com/?p=127093). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the module for visiting and reading websites.\n",
    "import urllib.request\n",
    "# Load the module for running regular expressions (regex).\n",
    "import re \n",
    "# Load the module for date and time stuff.\n",
    "import datetime\n",
    "# Define the variable now as equal to the current date and time.\n",
    "now = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the URL you want to scrape.\n",
    "url_1 = \"http://www.cnn.com/\"\n",
    "\n",
    "# If you want to scrape data from multiple pages, you can, \n",
    "# just replicate the above and below but change url_1 to url_2 et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the module for accessing Google Sheets.\n",
    "import gspread\n",
    "# Load the module needed for securely communicating with Google Sheets.\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "# The scope for your access credentials\n",
    "scope = ['https://spreadsheets.google.com/feeds']\n",
    "\n",
    "# Your spreadsheet's ID\n",
    "document_key = \"164Ae9PtqxwxTwO2l9y9qxtN4ZIpKdPLsjlHl5wDJOuI\" \n",
    "#              ^^^^^^^^^^^ SWAP OUT FOR YOUR DOCUMENT ID/KEY\n",
    "# Your Google project's .json key\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name('../../../../../SheetsBot-252f10f5aedd.json', scope)\n",
    "#                                                                              ^^^^^^^^ SWAP OUT FOR YOUR JSON KEY\n",
    "# Use your credentials to authorize yourself.\n",
    "gc = gspread.authorize(credentials)\n",
    "# Open up the Sheet with the defined ID.\n",
    "wks = gc.open_by_key(document_key)\n",
    "\n",
    "#########################################\n",
    "#\n",
    "#  NOTE: The name of the sheet you are \n",
    "#  trying to access should be in the \n",
    "#  parenthetical below (e.g., Data). By\n",
    "#  Default this is probably \"Sheet1\".\n",
    "#\n",
    "#########################################\n",
    "worksheet = wks.worksheet(\"Sheet1\")\n",
    "\n",
    "# Count the number of rows in your Sheet &\n",
    "# resize to remove blank rows.\n",
    "worksheet.resize(worksheet.row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the old values stored in your sheet \n",
    "# Note: The first time you run this code, it will be empty as nothing has yet to be stored in your sheet.\n",
    "\n",
    "print(worksheet.row_values(worksheet.row_count))\n",
    "#############################\n",
    "# DELETE CELL AFTER TESTING\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant Twitter libraries so you can use Twitter.\n",
    "import twitter\n",
    "from twitter import TwitterError\n",
    "\n",
    "# create the following four text files and add them to the same diretctry as you \n",
    "# Google API key. In each file add the appropriate value found when retrieving your \n",
    "# Twitter credentials\n",
    "\n",
    "with open('../../../../../key.txt', 'r') as myfile:\n",
    "    key=myfile.read()\n",
    "    \n",
    "with open('../../../../../secret.txt', 'r') as myfile:\n",
    "    secret=myfile.read()\n",
    "    \n",
    "with open('../../../../../token_key.txt', 'r') as myfile:\n",
    "    token_key=myfile.read()\n",
    "\n",
    "with open('../../../../../token_secret.txt', 'r') as myfile:\n",
    "    token_secret=myfile.read()\n",
    "\n",
    "# Set you Twitter API credentials.\n",
    "api = twitter.Api(consumer_key=key,\n",
    "                  consumer_secret=secret,\n",
    "                  access_token_key=token_key,\n",
    "                  access_token_secret=token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the contents of your first webpage\n",
    "\n",
    "When you run the next cell, your program will visit the first URL you defined above. It will then print out that page's HTML. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_1 = urllib.request.build_opener(urllib.request.HTTPCookieProcessor).open(url_1).read()\n",
    "print(p_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------\n",
    "\n",
    "# One Data Point, One Match\n",
    "\n",
    "## Parse the site's contents \n",
    "\n",
    "Scan the above HTML for the content you are trying to extract. Cut and paste the HTML above into the TEST STRING box over at [Regex 101](https://regex101.com/) and craft a regex that captures your desired content. Be sure to use the Python flavor.\n",
    "\n",
    "Remember the parenthetical is the group you're pulling out. Once you have a working regex, plug it into the code below, and run the cell. If it worked, you'll see you scraped data as an output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1 = re.search(b\"<title>(.*)</title>\",p_1)\n",
    "output_1 = res_1.group(1).decode('UTF-8')\n",
    "print(output_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post to Twitter and Save to Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (res_1 and (worksheet.row_values(worksheet.row_count)[1]) != output_1):\n",
    "    # The above If statment, says to continue only if the we actuall got some data from the page\n",
    "    # and the old sheet vales and the new pulled values are not equal (!=) to eachother. \n",
    "\n",
    "    # Go ahead and tweet out the update. Here you need to know about a Twitter API limitation.\n",
    "    # Twitter will not Tweet the same tweet a second time if it is too close to the first instance.\n",
    "    # In such cases, it will throw an error. The `try:` and `except TwitterError:` constructions are\n",
    "    # similar to If statements. However, they will try the first block of code first, and only try \n",
    "    # the second block if it runs into a Twitter error. Here, the second try tweaks the language\n",
    "    # just enough that it isn't a duplicate Tweet.\n",
    "    try:\n",
    "        # Post to Twitter.\n",
    "        #print ('P1 Title is %s'%(output_1))\n",
    "        status = api.PostUpdate('P1 Title is %s'%(output_1))\n",
    "        print(status.text)\n",
    "    except TwitterError:\n",
    "        # Post to Twitter.\n",
    "        #print ('P1 Title: %s'%(output_1))\n",
    "        status = api.PostUpdate('P1 Title: %s'%(output_1))\n",
    "        print(status.text)\n",
    "\n",
    "    # Save to Google only after Tweeting\n",
    "    worksheet.append_row([now,output_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(worksheet.row_values(worksheet.row_count))\n",
    "#############################\n",
    "# DELETE CELL AFTER TESTING\n",
    "#############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Data Points, One Match\n",
    "\n",
    "---------------------------\n",
    "## Parse the site's contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1 = re.search(b\"<a href=\\\"([^\\\"]*)\\\"[^>]*>([^<]*)</a>\",p_1)\n",
    "output_1 = res_1.group(1).decode('UTF-8')\n",
    "print(output_1)\n",
    "output_2 = res_1.group(2).decode('UTF-8')\n",
    "print(output_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post to Twitter and Save to Google (Two Data Point, One Match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (res_1 and (worksheet.row_values(worksheet.row_count)[1]) != output_1\n",
    "          and (worksheet.row_values(worksheet.row_count)[2]) != output_2):\n",
    "    # same as above but now comparing two values\n",
    "    \n",
    "    try:\n",
    "        # Post to Twitter.\n",
    "        status = api.PostUpdate('%s %s'%(output_2,output_1))\n",
    "        print(status.text)\n",
    "    except TwitterError:\n",
    "        # Post to Twitter.\n",
    "        status = api.PostUpdate('%s %s'%(output_2,output_1))\n",
    "        print(status.text)\n",
    "\n",
    "    # Save to Google only after Tweeting\n",
    "    worksheet.append_row([now,output_1,output_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(worksheet.row_values(worksheet.row_count))\n",
    "#############################\n",
    "# DELETE CELL AFTER TESTING\n",
    "#############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Data Points, Multiple Fixed Number of Matches\n",
    "\n",
    "-------------------------------------------\n",
    "\n",
    "## Parse the site's contents and post to Twitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = re.finditer(b\"<a href=\\\"([^\\\"]*)\\\"[^>]*>([^<]*)</a>\",p_1)\n",
    "column = 0\n",
    "columns = [now]\n",
    "\n",
    "for matchNum, match in enumerate(matches):\n",
    "    matchNum = matchNum + 1\n",
    "    column = column + 1\n",
    "    column2 = column + 1\n",
    "    \n",
    "    output_1 = match.group(1).decode('UTF-8')\n",
    "    output_2 = match.group(2).decode('UTF-8')\n",
    "    \n",
    "    if (res_1 and (worksheet.row_values(worksheet.row_count)[column]) != output_1 \n",
    "              and (worksheet.row_values(worksheet.row_count)[column2]) != output_2):\n",
    "        # same as above but now comparing the two values handled in this loop\n",
    "        \n",
    "        try:\n",
    "            # Post to Twitter.\n",
    "            status = api.PostUpdate('%s %s'%(output_2,output_1))\n",
    "            print(status.text)\n",
    "        except TwitterError:\n",
    "            # Post to Twitter.\n",
    "            status = api.PostUpdate('%s %s'%(output_2,output_1))\n",
    "            print(status.text)\n",
    "            \n",
    "        columns.append(output_1)\n",
    "        columns.append(output_2)\n",
    "        \n",
    "    column = column + 1\n",
    "\n",
    "# Save to Google only after Tweeting\n",
    "worksheet.append_row(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(worksheet.row_values(worksheet.row_count))\n",
    "#############################\n",
    "# DELETE CELL AFTER TESTING\n",
    "#############################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
